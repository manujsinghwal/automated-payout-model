{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pretty_html_table import build_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supportive Notebooks\n",
    "To calculate the successful sales events from these raw MIS, we need to parse these files from multiple functions. These function are defined in multiple notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing files for required data cleaning and processing\n",
    "%run py_base_files/savings_account.ipynb\n",
    "%run py_base_files/personal_loan.ipynb\n",
    "%run py_base_files/checker.ipynb\n",
    "%run py_base_files/mail_trigger.ipynb\n",
    "%run py_base_files/error_logs.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Supportive Files\n",
    "### Payin-Payout Rules\n",
    "There are some predefined rules to calculate the successful sale events. Based on these rules ShiftPay pays commision to the SPs. Basically these rules defines that how ShiftPay deducts a commission from sale amount and disburses the remaining funds to the respective Shift Partner.\n",
    "### Miscellaneous Leads\n",
    "Leads data which was generated by SPs using ShiftPay's application to sell the listed financial products. When we push this model to the production, this data will come from ShiftPay's database. Here, for simplicity of this project we have this data in a .csv file, in future this is going to be a lot of data and will not be feasiable to parse all misc_leads to the supportive files. For that we'll just need to pull the required leads data as per the MIS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading supportive files\n",
    "rules = pd.read_csv('supported_tables\\payin_payout_rules.csv')\n",
    "leads = pd.read_csv('supported_tables\\misc_leads.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw MIS\n",
    "For simplicity of this project, we're reading these .csv files from local machine. But when we push this model to production these files will be dropped to an Azure container by a specific team and this storage event will trigger a pipeline. That leads to run this notebook and we'll get the successful payout events files. Now the successful payout events files will be saved to another Azure container that will trigger another pipeline to upsert these records into database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the raw MIS from financial organization\n",
    "mis_bank_A_df = pd.read_csv('input_mis\\mis_bank_A.csv')\n",
    "mis_bank_B_df = pd.read_csv('input_mis\\mis_bank_B.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS and Supported File Parsing\n",
    "**Why data cleaning is required for these MIS?**\n",
    "\\\n",
    "ShiftPay observed in past, there were some instances in which we had the different data type for the success event columns in these MIS. That might lead to an error while processing these MIS. For example we setup our calculation based on YYYY-MM-DD format, but somehow we received the DD-MM-YYYY HH:MM:SS format. To deal with these situations we'll parse every MIS to the checker.ipynb notebook to ensure data cleaning in raw .csv files includes handling null values and dropping duplicates etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing MIS to the checker\n",
    "mis_bank_A_df = mis_bank_A_cleaner(mis_bank_A_df)\n",
    "mis_bank_B_df = mis_bank_B_cleaner(mis_bank_B_df)\n",
    "\n",
    "# Parsing supportive files to the checker\n",
    "rules = rules_cleaner(rules)\n",
    "leads = leads_cleaner(leads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Payout Calculation\n",
    "We're going to pass the cleaned MIS to defined product type notebooks for success event calculations. In py_base_files folder we defined multiple notebooks based on the product types. For every product type we need to calculate the success events differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing calculations to these MIS\n",
    "mis_bank_A_success = savings_account(mis_bank_A_df, rules, leads)\n",
    "mis_bank_B_success = personal_loan(mis_bank_B_df, rules, leads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Final Output Files\n",
    "Now we have the final output for succesful sale events. We're going to save these results to the output folder.\n",
    "\\\n",
    "\\\n",
    "**Note:**\n",
    "\\\n",
    "There are some chances we might encounter with an error while calculating these events To deal with these situations we performed the error handling while doing these calculations. In case of any error, our DataFrame will have a error str instead of DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mis_bank_A_success saved to .csv file successfully.\n",
      "mis_bank_B_success saved to .csv file successfully.\n"
     ]
    }
   ],
   "source": [
    "# Exporting final results\n",
    "error_check = [('mis_bank_A_success', mis_bank_A_success), \n",
    "    ('mis_bank_B_success', mis_bank_B_success)\n",
    "    ]\n",
    "\n",
    "for var_name, var_value in error_check:\n",
    "    if isinstance(var_value, pd.DataFrame):\n",
    "        # Saving DataFrame to .csv file\n",
    "        var_value.to_csv(f'output\\{var_name}.csv', index=False)\n",
    "        print(f'{var_name} saved to .csv file successfully.')\n",
    "\n",
    "    elif isinstance(var_value, str):\n",
    "        # Error handling\n",
    "        error_logs(var_name, var_value)\n",
    "        print(f'{var_name} encountered with an error. Please check the error_logs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of any error while iteration of this model, an error log will be appended to the error_log.csv file.\n",
    "\\\n",
    "\\\n",
    "Now, we have the successful events and their payin-payout respectively. These .csv files will be uploaded to the Azure container in production, and this storage event will trigger a pipeline that will upsert these records into the database and change the Status and SubStatus of misc_leads. Also payout will be updated to the SP's wallet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation\n",
    "Management would like to have an summary report for every iteration that should be pushed through an automated email. This report should contain the summary for success events and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Summary\n",
    "success_cases = [('mis_bank_A_success', mis_bank_A_success), \n",
    "    ('mis_bank_B_success', mis_bank_B_success)\n",
    "    ]\n",
    "\n",
    "success_df = pd.DataFrame({})\n",
    "current_date = datetime.now().date()\n",
    "\n",
    "# Concatination all the success events\n",
    "for var_name, var_value in error_check:\n",
    "    if isinstance(var_value, pd.DataFrame):\n",
    "        success_df = pd.concat(\n",
    "            [var_value, success_df],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "# Summary metrics\n",
    "created_at = current_date\n",
    "\n",
    "success_summary = (\n",
    "    success_df\n",
    "    .groupby(['SourceType', 'MediumType', 'ProductType'])\n",
    "    .agg(\n",
    "        UniqueSP = ('SPId', 'nunique'),\n",
    "        TotalLeads = ('LeadId', 'count'),\n",
    "        TotalPayin = ('TotalPayin', 'sum'),\n",
    "        TotalPayout = ('TotalPayout', 'sum')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "success_summary['CreatedAt'] = current_date\n",
    "success_summary['Revenue'] = success_summary['TotalPayin'] - success_summary['TotalPayout']\n",
    "success_summary = success_summary[['CreatedAt','SourceType','MediumType','ProductType','UniqueSP','TotalLeads','TotalPayin','TotalPayout','Revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Summary\n",
    "errors = pd.read_csv('output\\error_logs.csv')\n",
    "errors = errors_cleaner(errors)\n",
    "\n",
    "# Filtering the errors for current iteration\n",
    "errors = (\n",
    "    errors\n",
    "    .query('CreatedAt == @current_date & CreatedAt == CreatedAt.max()')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully!\n"
     ]
    }
   ],
   "source": [
    "# Final HTML summary\n",
    "body_1 = '''\n",
    "<p>Hey team,</p>\n",
    "<p>Please find the below success summary for the current iteration of MIS automation.</p><br>\n",
    "'''\n",
    "body_2 = build_table(success_summary, 'blue_light')\n",
    "\n",
    "# Concatenating the above results \n",
    "success_body = body_1 + body_2\n",
    "\n",
    "# Checking for erros\n",
    "if len(errors) > 0:\n",
    "    body_3 = '''\n",
    "    <br><p>We encountered with an error while processing the below MIS. Please take a look on error_logs to know more about the exact errors.</p><br>\n",
    "    '''\n",
    "    body_4 = build_table(errors, 'red_light')\n",
    "    error_body = body_3 + body_4\n",
    "else:\n",
    "    error_body = '''\n",
    "    <br><p>No error were detected during this iteration.</p><br>\n",
    "    '''\n",
    "body_5 = '''\n",
    "<p>Thanks,</p>\n",
    "<p>ShiftPay Analytics Team</p>\n",
    "'''\n",
    "\n",
    "final_body = success_body + error_body + body_5\n",
    "email(final_body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
